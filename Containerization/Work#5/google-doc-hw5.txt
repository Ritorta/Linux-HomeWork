Google-doc HomeWork#5 Containerization - Урок 5. Docker Compose и Docker Swarm

Конкретно в моём случае были проблемы с VRBox который был программой для эмуляции систем Linux, не работает сетевой мост,
который важен для создания сети и раздачи интернета.
Данная проблема была решина, костылём в виде выбора виртуального адаптера, с переодическим переключением на NAT когда,
требовалось интернет подключение.
Ещё как вариант можно воспользоваться арендой виртуальных машин Linux за деньги на специализированных сайтах.
Стандартно подключение должно проводиться через сетевой мост и никаких дополнительных проблем и манипуляций не требуется.

Задания: 

1. Создать сервис, состоящий из 2 различных контейнеров: 1 - веб, 2 - БД.

Выполнение:

Для этого создаём документ compose.yaml

version: '3.9'

services:

     db:
       image: mariadb:10.10.2
       restart: always
       environment:
          MYSQL_ROOT_PASSWORD: 12345

     adminer:
       image: adminer:4.8.1
       restart: always
       ports:
          6080:8080



2. Далее необходимо создать 3 сервиса в каждом окружении (dev, prod, lab).

Выполнение:

Для этого создаём документы:
--- compose.dev.yaml ---

version: '3.9'

services:

  db:
    image: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: 12345

  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    restart: always
    ports:
       80:80
    environment:
      MYSQL_ROOT_PASSWORD: 12345

--- compose.lab.yaml ---

version: '3.9'

services:

  db:
    image: mysql:latest
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: 12345

  web:
    image: nginx:latest
    restart: always
    ports:
       80:80
    volumes:
    	./src:/usr/share/nginx/html
    links: 
    	php

--- compose.prod.yaml ---

version: '3.9'
services:

     db:
       image: mariadb:10.10.2
       restart: always
       environment:
          MYSQL_ROOT_PASSWORD: 12345

     adminer:
       image: adminer:4.8.1
       restart: always
       ports:
          6080:8080

3. По итогу на каждой ноде должно быть по 2 работающих контейнера.

Выполнение:

Добавляем в .yaml:

 deploy:
      replicas: 2

Для запуска под два экземпляра на ноде.


4. Выводы зафиксировать.

Выполнение:

Для того чтобы пользоваться swarm, надо запомнить несколько типов сущностей:

Node - это наши виртуальные машины, на которых установлен docker. 
Есть manager и workers ноды. Manager нода управляет workers нодами. 
Она отвечает за создание/обновление/удаление сервисов на workers, 
а также за их масштабирование и поддержку в требуемом состоянии. 
Workers ноды используются только для выполнения поставленных задач и не могут управлять кластером.

Stack - это набор сервисов, которые логически связаны между собой. 
По сути это набор сервисов, которые мы описываем в обычном compose файле. 
Части stack (services) могут располагаться как на одной ноде, так и на разных.

Service - это как раз то, из чего состоит stack. Service является описанием того, 
какие контейнеры будут создаваться. Если вы пользовались docker-compose.yaml, 
то уже знакомы с этой сущностью. Кроме стандартных полей docker в режиме swarm поддерживает ряд дополнительных, 
большинство из которых находятся внутри секции deploy.

Task - это непосредственно созданный контейнер, который docker создал на основе той информации, 
которую мы указали при описании service. 
Swarm будет следить за состоянием контейнера и при необходимости его перезапускать или перемещать на другую ноду.

Для того чтобы сервисы распределять на определенные ноды необходимо применять labels.

6* Нужно создать 2 ДК-файла, в которых будут описываться сервисы.

Выполнение:

Для этого создаём два документа:

--- compose.yaml --- 

version: '3.9'

services:

     db:
       image: mariadb:10.10.2
       restart: always
       environment:
          MYSQL_ROOT_PASSWORD: 12345

     adminer:
       image: adminer:4.8.1
       restart: always
       ports:
          6080:8080

--- compose.yml ---

version: '3.9'
services:

  db:
    image: mysql:8.3.0
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: 12345

  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    restart: always
    ports:
       80:80
    environment:
      MYSQL_ROOT_PASSWORD: 12345

7* Повторить задание 1(6* - в моём формате документа) для двух окружений: lab, dev

Выполнение:

Создадим две папки и повторим для ниж задание 1(6* - в моём формате документа), поскольку это повторение действий выше только в разных папках подробного описания не будет.

8* Обязательно проверить и зафиксировать результаты, чтобы можно было выслать преподавателю для проверки

Выполнение:

Описано в пункте 6* и 7*.

9. Персональное задание от преподавателя повторить действия на семенаре как по методичке:

9.1 Создать ДК файл декларативным и императивным подходом.

Выполнение:

Создаём docker-compose.yaml файл с нашим сервисом(декларативный подход):

version: '3.9'

services:

  db:
    image: mysql
    environment:
      MYSQL_ROOT_PASSWORD: 12345
    networks:
      - dbui_network
    hostname: db

  ui:
    image: phpmyadmin
    ports:
      - 8081:80
    networks:
      - dbui_network
    hostname: ui

networks:
  dbui_network:

Или спомощью bash скрипта файла(императивный подход):

#!/bin/bash
(docker rm -vf db ui - команда для автоматического удаления контейнеров при пересоздании)
docker run -d -e MYSQL_ROOT_PASSWORD=12345 --hostname db --name db mysql
docker run -d -p 80:80 --link db:db --hostname ui --name ui phpmyadmin

---- Creanshot 1 HW5----

9.2 Развернуть Docker-Swarm.

Клонируем Lunux до трёх штук, настраиваем сетевой мост, запускаем и подключаемся к всем трёх системам через ssh,
переименовываем хосты двух машин для орентирования(нужно перезайти по ssh чтобы изменения хоста вступили в силу).

> ssh fox@192.168.0.150
fox@love-linux:~$ sudo hostnamectl set-hostname node-3
[sudo] password for fox:
fox@love-linux:~$ exit
> ssh fox@192.168.0.150
fox@node-3:~$

---- Creanshot 2 HW5----

Теперь инициализируем кластер на первой машине создаём мастер машину а затем добавляем две другие машини как воркеров, 
вставляя в консоль каждой машины команду для поключения к мастеру.

fox@love-linux:~$ sudo docker swarm init
fox@node-2:~$ docker swarm join --token SWMTKN-1-562u6nj0h8k800sdvwpqc37zavmd2ssyxcsq651mtl4qv6qjmj-9a9unuyx4h39k3t2vogcz6bna 192.168.0.193:2377
This node joined a swarm as a worker.
fox@node-3:~$ docker swarm join --token SWMTKN-1-562u6nj0h8k800sdvwpqc37zavmd2ssyxcsq651mtl4qv6qjmj-9a9unuyx4h39k3t2vogcz6bna 192.168.0.193:2377
This node joined a swarm as a worker.

Теперь проверим командой наличие нодов в листе на оснвоной машине.

fox@love-linux:~$ sudo docker node ls
ID                            HOSTNAME     STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
k7dnyxp21cw6n2qlsdkxva8cj     love-linux   Ready     Active                          24.0.7
tivwu86e4e2c0gtnamj9eszd9 *   love-linux   Ready     Active         Leader           24.0.7
v7heyztfb9p862lpqopvhn43g     love-linux   Ready     Active                          24.0.7

---- Creanshot 3 HW5----

9.3 Создаём сначала одну а затем несколько служб nginx.

Командой ниже скачаем и настроим образ nginx:
fox@love-linux:~$ docker service create --name nginx -p 80:80 nginx 

Этой командой запустим его несколько раз, нужно учесть что без интернета swarm не сможет запустить nginx на других машинах,
другие машины в среде swarm должны иметь образ nginx,
в виду чего запуск будет доступен только на тех машинах на которых есть образ nginx.
fox@love-linux:~$ docker service scale nginx=9

На скриншите видим, что swarm равномерно распределил нагрузку между нодами.

---- Creanshot 4 HW5----

9.4 Изменить файл запускаемого nginx чтобы определять какая нода отвечает за процесс nginx (подписываем вирнутуальные машины прям на сайте).

Для этого введём следующие команды:

Для начала заходим в демон nginx

fox@love-linux:~$ docker exec -it c bash (где "с" первая буква id демона)

Далее переходим в папку место нахождения файла конфинга index.html:

root@c9bbb1686f22:/usr/local/share# cd /usr/local/share/nginx/html/ 

Далее изменим содержимое для того, чтобы обозначить наши ноды:

root@c9bbb1686f22:/usr/share/nginx/html# echo thes master-node > index.html
root@c9bbb1686f22:/usr/share/nginx/html# cat index.html
thes master-node

Теперь проделаем подобное для каждой ноды, стоит обратить внимание, что данный процесс нужно проделать для каждого демона, 
так что будет использовать длинную команду с подставлением id демона или как вариант можно написать bash скрип для автоматизации процесса:

fox@love-linux:~$ docker exec 2f bash -c "echo node-0-1 > /usr/share/nginx/html/index.html" 
(В данном случае было испольщовано более одного символа для обозначения id в виду сопадения первого символя у демонов)

---- Creanshot 5 HW5----

Далее исспользуя команду curl мы видим, что идёт автоматическое перераспределение нагрузки, в моём случае только
на две из трёх ввиду проблем с соединеним, одна машина в режиме NAT хоть и запускает и является частью swarm но,
очень коректно работает в рамках сети распределения.

fox@love-linux:~$ curl 192.168.56.102

---- Creanshot 6 HW5----

9.5 Теперь попробуем запустить через docker-compose для этого перейдём в каталог с нашим compose.yaml файлом и выполним команду:

fox@love-linux:~/HW5/T4$ docker stack up -c compose.yaml dbui1

На двух из трёх нодах запустились контейнеры, что видно на скриншоте:

---- Creanshot 7 HW5----

Опять же ввиду того, что третья нода сидит на пседво NAT сети и на ней живёт db к ней полноценно не обратиться она полу изолирована.

9.6 Доп. найстройка Docker для работы с Kubernetes

В различных источниках рекомендуется перейти именно на драйвер systemd, как на более современный для работы с Kubernetes

Для этого перейдём в папку /etc/docker и создадим в ней файл daemon.json и пропишем туда:

{
  "exec-opts": ["native.cgroupdriver=systemd"]
}

fox@love-linux:~$ sudo nano /etc/docker/daemon.json

Проверяем, что изменения были успешно внесены и файл создан.

fox@node-3:~$ sudo cat /etc/docker/daemon.json

9.6.1 Предварительная настройка для Kubernetes

Первое дать имена хостам этой командой:

fox@love-linux:~$ sudo hostnamectl set-hostname master-node

Потом надо поменять отредактировать host файл:

fox@love-linux:~$ sudo nano /etc/hosts

Добавить ниже подобную строку где IP ваш статический IP и имя вашего хоста. 
Обращаю конкретное внимание, что с динамическим IP Kubernetes работать не будет, IP должен быть статическим!

192.168.56.104	love-linux 


(IP можно посмотреть командой "ip addr" или "ip a")

Далее обновляем дополнительные пакеты и утилиты.
В процессе установки iptables-persistent может запросить подтверждение сохранить правила брандмауэра — отказываемся.

sudo apt-get update && apt-get upgrade -y

sudo apt-get install curl apt-transport-https git iptables-persistent -y

Затем отключаем файл подкачки swap, из-за которого Kubernetes не включиться, предварительно проверямем включен ли он:

fox@love-linux:~$ sudo swapoff -a

Чтобы он не включился после перезагрузки, необходимо изменить файл fstab:

fox@love-linux:~$ sudo nano /etc/fstab

Закомментируем (или просто удалим) последнюю строку (в которой упоминается файл /swap.img), сохраним файл и перезагрузим виртуальную машину
Swap надо выключить для каждой виртуальной машины.

/swap.img      none    swap    sw      0       0

Проверяем:

fox@love-linux:~$ cat /proc/swaps

Если файлы есть, то swap включен, поэтому переходим в папку /etc, ищем там файл fstab и открываем его на редактирование и проверяем.

Потом загрузим дополнительные модули ядра:

sudo nano /etc/modules-load.d/k8s.conf

В k8s.conf добавим 2 строки:

br_netfilter
overlay

Теперь добавим модули в ядро:

sudo modprobe br_netfilter
sudo modprobe overlay

Проверим успешность действий:

sudo lsmod | egrep "br_netfilter|overlay"

Должен быть примерно такой вывод:

overlay               147456  0
br_netfilter           28672  0
bridge                299008  1 br_netfilter

Создадим конфигурационный файл для обработки трафика через bridge в netfilter:

sudo nano /etc/sysctl.d/k8s.conf

В файл вносим 2 строки:

net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

Применим настройки:

sudo sysctl --system

9.6.2 Установка Kubernetes 

Добавим GPG-ключ:

sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

Теперь создадим файл с настройкой репозитория:

sudo nano /etc/apt/sources.list.d/kubernetes.list

И добавим в него такую запись:

deb https://apt.kubernetes.io/ kubernetes-xenial main

Обновим пакеты и устанавливаем:

sudo apt-get update
sudo apt-get install kubelet kubeadm kubectl

Првоерка версии:

sudo kubectl version --client 

9.6.3 Доп. настройка для после установки Kubernetes

Изменяем cgroup driver для Docker и Kubernetes на systemd, 
в различных источниках рекомендуется перейти именно на драйвер systemd, как на более современный. 

Для изменения драйвера в Kubernetes отредактируем файл /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
И добавим в конец строки ExecStart=/usr/bin/kubelet... ещё один параметр --cgroup-driver=systemd

fox@love-linux:~$ cd /etc/systemd/system/kubelet.service.d/
fox@love-linux:/etc/systemd/system/kubelet.service.d$ sudo nano 10-kubeadm.conf

Изменяем cgroup driver для Docker и Kubernetes на systemd, 
в различных источниках рекомендуется перейти именно на драйвер systemd, как на более современный. 

Для изменения драйвера в Kubernetes отредактируем файл /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
И добавим в конец строки ExecStart=/usr/bin/kubelet... ещё один параметр --cgroup-driver=systemd

fox@love-linux:~$ cd /etc/systemd/system/kubelet.service.d/
fox@love-linux:/etc/systemd/system/kubelet.service.d$ sudo nano 10-kubeadm.conf

9.7 Настройка и создание кластера  Kubernetes

Выполняем команду для начальной настройки и подготовки узла:
Параметр --apiserver-advertise-address отвечает за то, по какому IP адресу будет доступен apiserver.
Без него будет использоваться IP адрес сетевого адаптера по умолчанию, которым является NAT, 
и мы не сможем присоединить узлы к master узлу. 
Параметр --pod-network-cidr указывает подсеть, в которой подам будут выделяться виртуальные IP адреса внутри кластера.
Главное, чтобы эта подсеть не пересекалась с другими подсетями, используемыми виртуальными машинами.

sudo kubeadm init \
	--apiserver-advertise-address=192.168.10.10 \ 
  --pod-network-cidr=10.10.0.0/16

Ключ --pod-network-cidr задает адрес внутренней подсети. 10.244.0.0/16 — значение по умолчанию.

Процесс займет несколько минут(У меня занял 20 минут). По его завершению в консоли появится ключ для подключения workers:

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.2.15:6443 --token....

Создадим переменную окружения KUBECONFIG:

export KUBECONFIG=/etc/kubernetes/admin.conf

Чтобы каждый раз при входе в систему не приходилось повторять данную команду, открываем файл: 

nano /etc/environment

И добавляем в него строку: 

export KUBECONFIG=/etc/kubernetes/admin.conf

Посмотреть список узлов кластера можно командой:

sudo kubectl get nodes

Также выполним установку CNI (сетевой интерфейс контейнеров):

sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

9.8 Подключение рабочих нод

Данная команда позволит ещё раз вывести токен для подключения рабочих нод:

sudo kubeadm token create --print-join-command

После чего копируем ключ на другие ноды и поключаем их.

Командой можно проверить список подключённых нод:

fox@love-linux:~$ sudo kubectl get nodes

NAME         STATUS   ROLES           AGE     VERSION
love-linux   Ready    control-plane   29m     v1.28.2
node-2       Ready    <none>          19m     v1.28.2
node-3       Ready    <none>          4m57s   v1.28.2

9.9 Создание пода

Поды создаются командой kubectl:

fox@love-linux:~$ sudo kubectl run nginx --image=nginx:latest --port=80

pod/nginx created

Чтобы получить сетевой доступ к созданному поду, создаем port-forward следующей командой:

kubectl port-forward nginx --address 0.0.0.0 8888:80


Поды не поднимаются, сил моих больше нет на эту программу лучше пользуйтесь Docker там всё отлично работает и без кучи плясок с бубном.



And the soution is to edit /etc/containerd/config.toml to have:

version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
   [plugins."io.containerd.grpc.v1.cri".containerd]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true

Then restart the containerd service:

sudo systemctl restart containerd

sudo kubeadm reset

sudo kubeadm init

https://www.dmosk.ru/instruktions.php?object=kubernetes-ubuntu&ysclid=lrrgidhwwt654702078

https://github.com/kubernetes/kubeadm/issues/2833?ysclid=lrrib6a3nc288839223

https://habr.com/ru/articles/530352/

https://habr.com/ru/articles/542042/