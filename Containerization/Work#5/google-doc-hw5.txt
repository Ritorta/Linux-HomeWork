Google-doc HomeWork#5 Containerization - Урок 5. Docker Compose и Docker Swarm

Конкретно в моём случае были проблемы с VRBox который был программой для эмуляции систем Linux, не работает сетевой мост,
который важен для создания сети и раздачи интернета.
Данная проблема была решина, костылём в виде выбора виртуального адаптера, с переодическим переключением на NAT когда,
требовалось интернет подключение.
Ещё как вариант можно воспользоваться арендой виртуальных машин Linux за деньги на специализированных сайтах.
Стандартно подключение должно проводиться через сетевой мост и никаких дополнительных проблем и манипуляций не требуется.

Задания: 

1. Создать сервис, состоящий из 2 различных контейнеров: 1 - веб, 2 - БД.

Выполнение:

Для этого создаём документ compose.yaml

version: '3.9'

services:
     db:
       image: mariadb:10.10.2
       restart: always
       environment:
          MYSQL_ROOT_PASSWORD: 12345

     adminer:
       image: adminer:4.8.1
       restart: always
       ports:
          6080:8080

networks:
  app-dk-1:


2. Далее необходимо создать 3 сервиса в каждом окружении (dev, prod, lab).

Выполнение:

Для этого создаём документы:
--- compose.dev.yaml ---

version: '3.9'

services:
  db:
    image: mysql:8.3.0
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: 12345

  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    restart: always
    ports:
       80:81
    environment:
      MYSQL_ROOT_PASSWORD: 12345

--- compose.lab.yaml ---

version: '3.9'

services:
  db:
    image: mysql:8.3.0
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: 12345

  web:
    image: nginx:latest
    restart: always
    ports:
       80:82
    volumes:
    	./src:/usr/share/nginx/html
    links: 
    	php

--- compose.prod.yaml ---

version: '3.9'
services:
     db:
       image: mariadb:10.10.2
       restart: always
       environment:
          MYSQL_ROOT_PASSWORD: 12345

     adminer:
       image: adminer:4.8.1
       restart: always
       ports:
          6080:8080

3. По итогу на каждой ноде должно быть по 2 работающих контейнера.

Выполнение:

Добавляем в .yaml:

 deploy:
      replicas: 2

Для запуска под два экземпляра на ноде.


4. Выводы зафиксировать.

Выполнение:

Для того чтобы пользоваться swarm, надо запомнить несколько типов сущностей:

Node - это наши виртуальные машины, на которых установлен docker. 
Есть manager и workers ноды. Manager нода управляет workers нодами. 
Она отвечает за создание/обновление/удаление сервисов на workers, 
а также за их масштабирование и поддержку в требуемом состоянии. 
Workers ноды используются только для выполнения поставленных задач и не могут управлять кластером.

Stack - это набор сервисов, которые логически связаны между собой. 
По сути это набор сервисов, которые мы описываем в обычном compose файле. 
Части stack (services) могут располагаться как на одной ноде, так и на разных.

Service - это как раз то, из чего состоит stack. Service является описанием того, 
какие контейнеры будут создаваться. Если вы пользовались docker-compose.yaml, 
то уже знакомы с этой сущностью. Кроме стандартных полей docker в режиме swarm поддерживает ряд дополнительных, 
большинство из которых находятся внутри секции deploy.

Task - это непосредственно созданный контейнер, который docker создал на основе той информации, 
которую мы указали при описании service. 
Swarm будет следить за состоянием контейнера и при необходимости его перезапускать или перемещать на другую ноду.

Для того чтобы сервисы распределять на определенные ноды необходимо применять labels.

6* Нужно создать 2 ДК-файла, в которых будут описываться сервисы.

Выполнение:

Для этого создаём два документа:

--- compose.yaml --- 

version: '3.9'

services:
     db:
       image: mariadb:10.10.2
       restart: always
       environment:
          MYSQL_ROOT_PASSWORD: 12345

     adminer:
       image: adminer:4.8.1
       restart: always
       ports:
          6080:8080

--- compose.yml ---

version: '3.9'
services:

  db:
    image: mysql:8.3.0
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: 12345

  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    restart: always
    ports:
       80:80
    environment:
      MYSQL_ROOT_PASSWORD: 12345

7* Повторить задание 1(6* - в моём формате документа) для двух окружений: lab, dev

Выполнение:

Создадим две папки и повторим для ниж задание 1(6* - в моём формате документа), поскольку это повторение действий выше только в разных папках подробного описания не будет.

8* Обязательно проверить и зафиксировать результаты, чтобы можно было выслать преподавателю для проверки

Выполнение:

Описано в пункте 6* и 7*.

9. Персональное задание от преподавателя повторить действия на семенаре как по методичке:

9.1 Создать ДК файл декларативным и императивным подходом.

Выполнение:

Создаём docker-compose.yaml файл с нашим сервисом(декларативный подход):

version: '3.9'

services:
  db:
    image: mysql
    environment:
      MYSQL_ROOT_PASSWORD: 12345
    networks:
      - dbui_network
    hostname: db

  ui:
    image: phpmyadmin
    ports:
      - 8081:80
    networks:
      - dbui_network
    hostname: ui

networks:
  dbui_network:

Или спомощью bash скрипта файла(императивный подход):

#!/bin/bash
(docker rm -vf db ui - команда для автоматического удаления контейнеров при пересоздании)
docker run -d -e MYSQL_ROOT_PASSWORD=12345 --hostname db --name db mysql
docker run -d -p 80:80 --link db:db --hostname ui --name ui phpmyadmin

---- Creanshot 1 HW5----

9.2 Развернуть Docker-Swarm.

Клонируем Lunux до трёх штук, настраиваем сетевой мост, запускаем и подключаемся к всем трёх системам через ssh,
переименовываем хосты двух машин для орентирования(нужно перезайти по ssh чтобы изменения хоста вступили в силу).

> ssh fox@192.168.0.150
fox@love-linux:~$ sudo hostnamectl set-hostname node-3
[sudo] password for fox:
fox@love-linux:~$ exit
> ssh fox@192.168.0.150
fox@node-3:~$

---- Creanshot 2 HW5----

Теперь инициализируем кластер на первой машине создаём мастер машину а затем добавляем две другие машини как воркеров, 
вставляя в консоль каждой машины команду для поключения к мастеру.

fox@love-linux:~$ sudo docker swarm init
fox@node-2:~$ docker swarm join --token SWMTKN-1-562u6nj0h8k800sdvwpqc37zavmd2ssyxcsq651mtl4qv6qjmj-9a9unuyx4h39k3t2vogcz6bna 192.168.0.193:2377
This node joined a swarm as a worker.
fox@node-3:~$ docker swarm join --token SWMTKN-1-562u6nj0h8k800sdvwpqc37zavmd2ssyxcsq651mtl4qv6qjmj-9a9unuyx4h39k3t2vogcz6bna 192.168.0.193:2377
This node joined a swarm as a worker.

Теперь проверим командой наличие нодов в листе на оснвоной машине.

fox@love-linux:~$ sudo docker node ls
ID                            HOSTNAME     STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
k7dnyxp21cw6n2qlsdkxva8cj     love-linux   Ready     Active                          24.0.7
tivwu86e4e2c0gtnamj9eszd9 *   love-linux   Ready     Active         Leader           24.0.7
v7heyztfb9p862lpqopvhn43g     love-linux   Ready     Active                          24.0.7

---- Creanshot 3 HW5----

9.3 Создаём сначала одну а затем несколько служб nginx.

Командой ниже скачаем и настроим образ nginx:
fox@love-linux:~$ docker service create --name nginx -p 80:80 nginx 

Этой командой запустим его несколько раз, нужно учесть что без интернета swarm не сможет запустить nginx на других машинах,
другие машины в среде swarm должны иметь образ nginx,
в виду чего запуск будет доступен только на тех машинах на которых есть образ nginx.
fox@love-linux:~$ docker service scale nginx=9

На скриншите видим, что swarm равномерно распределил нагрузку между нодами.

---- Creanshot 4 HW5----

9.4 Изменить файл запускаемого nginx чтобы определять какая нода отвечает за процесс nginx (подписываем вирнутуальные машины прям на сайте).

Для этого введём следующие команды:

Для начала заходим в демон nginx

fox@love-linux:~$ docker exec -it c bash (где "с" первая буква id демона)

Далее переходим в папку место нахождения файла конфинга index.html:

root@c9bbb1686f22:/usr/local/share# cd /usr/local/share/nginx/html/ 

Далее изменим содержимое для того, чтобы обозначить наши ноды:

root@c9bbb1686f22:/usr/share/nginx/html# echo thes master-node > index.html
root@c9bbb1686f22:/usr/share/nginx/html# cat index.html
thes master-node

Теперь проделаем подобное для каждой ноды, стоит обратить внимание, что данный процесс нужно проделать для каждого демона, 
так что будет использовать длинную команду с подставлением id демона или как вариант можно написать bash скрип для автоматизации процесса:

fox@love-linux:~$ docker exec 2f bash -c "echo node-0-1 > /usr/share/nginx/html/index.html" 
(В данном случае было испольщовано более одного символа для обозначения id в виду сопадения первого символя у демонов)

---- Creanshot 5 HW5----

Далее исспользуя команду curl мы видим, что идёт автоматическое перераспределение нагрузки, в моём случае только
на две из трёх ввиду проблем с соединеним, одна машина в режиме NAT хоть и запускает и является частью swarm но,
очень коректно работает в рамках сети распределения.

fox@love-linux:~$ curl 192.168.56.102

---- Creanshot 6 HW5----

9.5 Теперь попробуем запустить через docker-compose для этого перейдём в каталог с нашим compose.yaml файлом и выполним команду:

fox@love-linux:~/HW5/T4$ docker stack up -c compose.yaml dbui1

На двух из трёх нодах запустились контейнеры, что видно на скриншоте:

---- Creanshot 7 HW5----

Опять же ввиду того, что третья нода сидит на пседво NAT сети и на ней живёт db к ней полноценно не обратиться она полу изолирована.

9.6 Доп. найстройка Docker для работы с Kubernetes

В различных источниках рекомендуется перейти именно на драйвер systemd, как на более современный для работы с Kubernetes

Для этого перейдём в папку /etc/docker и создадим в ней файл daemon.json и пропишем туда:

{
  "exec-opts": ["native.cgroupdriver=systemd"]
}

fox@love-linux:~$ sudo nano /etc/docker/daemon.json

Проверяем, что изменения были успешно внесены и файл создан.

fox@node-3:~$ sudo cat /etc/docker/daemon.json

9.6.1 Установка и настройка Kubernetes

В гайде по установке данной программы был обнаружен способ по исправлению проблем при работе с NAT и сетью но уже слишком поздно,
переделывать по 20 раз я уже не будут там и так всё отлично работает пусть и на две с половиной ноды.
Зато в с Kubernetes не должно быть проблем надеюсь.

И так для начала уставноим его есть два способа использую офицальный гайд с офицального сайта или ниже приведённый скрипт.

#!/usr/bin/env bash

sudo apt-get update

sudo apt-get install -y \
    apt-transport-https \
    curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

fox@love-linux:~$ sudo chmod +x kiber.sh 
fox@love-linux:~$ ./kiber.sh

Изменяем cgroup driver для Docker и Kubernetes на systemd, 
в различных источниках рекомендуется перейти именно на драйвер systemd, как на более современный. 

Для изменения драйвера в Kubernetes отредактируем файл /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
И добавим в конец строки ExecStart=/usr/bin/kubelet... ещё один параметр --cgroup-driver=systemd

fox@love-linux:~$ cd /etc/systemd/system/kubelet.service.d/
fox@love-linux:/etc/systemd/system/kubelet.service.d$ sudo nano 10-kubeadm.conf

Когда Kubernetes будет запускаться на узле, он в качестве IP узла будет использовать IP адрес сетевого адаптера,
по умолчанию, а это NAT с динамическими адресами, который нам не подходит. Чтобы использовать статический IP адрес, 
который мы указали, отредактируем файл /etc/systemd/system/kubelet.service.d/10-kubeadm.conf,
добавим в конец строки ExecStart=/usr/bin/kubelet... ещё один параметр --node-ip=192.168.56.104 (текущий ip адрес с сетевого адаптера)

После перезагружаем виртуальные машины (нужно проделать для каждой).

9.7 Создание нодов через Kubernetes

sudo kubeadm init \
	--apiserver-advertise-address=192.168.58.104 \
  --pod-network-cidr=10.10.0.0/16 > ~/shared/kubeadm-join.sh

  получаем ошибку:
  
  [ERROR CRI]: container runtime is not running: output: time="2024-01-23T12:34:22+03:00" level=fatal msg="validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
  
   и идём опять курить кучу гайдов очень "люблю" линукс )